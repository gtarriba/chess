---
title: "Chess games analysis"
author: "Gabriel Tarriba"
date: "5/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(reticulate)
library(ggplot2)
library(pastecs)
```

# Analysis of chess results

In this document I analyse a dataset with the results of over 20,000 chess games played on [lichess](https://lichess.org/), one of the most popular platforms to play chess online. The dataset is on [Kaggle](https://www.kaggle.com/datasnaek/chess) and was uploaded by user [Mitchell J](https://www.kaggle.com/datasnaek). All the analysis is performed in [R](https://www.r-project.org/about.html).

I will perform Exploratory Data Analysis as well as some simple Regression Analysis/Machine Learning to find out:

1. What is in the data 
2. How large white's advantage is
3. How rating difference relates to game length (in number of turns)
4. How the rating difference between the two players predicts the outcome of the game 
5. What predicts whether a game will end by checkmate, resignation or time out

# 1. What is in the data: Load the dataset, clean it, select relevant variables

Examine the structure of the dataset, the number of observations and the presence of missing values. We see that there are 16 variables and 20,058 observations. The dataset has no missing values, which is great. We keep only 8 variables relevant for our analysis:

* rated - Whether the game was rated or not
* turns - Total number of moves (sum of both players)
* victory_status - How the game ended (time-out, resignation, mate, draw)
* winner (black or white, or draw)
* white_rating - On [lichess.com's Glicko 2 scale](https://en.wikipedia.org/wiki/Glicko_rating_system)
* black_rating - Same scale as white_rating

```{r}
# Load the data
full <- read.csv("games.csv")

# Remove null columns
full <- full [ ,colSums(is.na(full))<nrow(full)]

# Remove NA's
full <- full [complete.cases(full), ]

# Keep relevant variables
df <- select(full, rated, turns, victory_status, winner, white_rating, black_rating)

# Check structure of dataset
str(df)

# Create factors
names <- c('rated' ,'victory_status', 'winner')
df[,names] <- lapply(df[,names] , factor)

# The 'rated' variable has 4 levels because of different spellings (i.e. False and FALSE), this needs to be fixed
df$rated <- gsub("False", "FALSE", df$rated)
df$rated <- gsub("True", "TRUE", df$rated)

# Descriptive stats of numeric vars
descriptive <- stat.desc(df[ , (c(2, 5:6))])
round(descriptive, 2)
```

## 2. How large is the white advantage

White has the first-mover advantage in chess. But just how significant this really is? How much more does white win compared to black? We see that black wins about 45% of times, and white about 50%. This indicates that a white victory is 4.46 percentage points more likely. However, since black only wins 45% of times, a 4.46 percentage point difference in favor of white means that white is about 9% likelier to win than black!

```{r}
# Who wins how often?
perc_win <- table(df$winner)/nrow(df)*100
print(perc_win)

# Percentage point difference in prob. of white win
perc_win[3] - perc_win[1]

# How much likelier is a white win than a black win?
(perc_win[3] - perc_win[1])/perc_win[3]

# In a chart, by rated
df %>% 
  group_by(rated, winner) %>% 
  summarize(n = n()) %>% 
  mutate(prop = n/sum(n)) %>% 
  ggplot(aes(fill=winner, y=prop, x=rated)) + 
    geom_bar(position="fill", stat="identity")
```

## 3. How do rating difference relates to game length (in number of turns)

Now we use graphs to inspect the distribution of (1) rating differences, (2) the number of turns in a game (i.e. the sum of both players' moves), and (3) how these two are related. But first we need to construct the variable "rating differences", as the absolute value of the ratings. We see that most games are played between rivals of roughly similar strength, which is unsurprising as this is how lichess' algorithm operates by default. The median rating difference is 173 whereas the median game length is 55 turns. Do people with more similar ratings play longer games?

```{r}
# Variable of rating differences
df$diff <- with(df, abs(white_rating - black_rating))

# Summary stats for the two variables of interest
summary(df$diff)
summary(df$turns)

# Histogram of rating differences
hist(df$diff, breaks=50)

# Histogram of game length
hist(df$turns, breaks=50)

# Scatterplot of rating difference and turns
ggplot(df, aes(x=diff, y=turns)) + geom_point(alpha=0.3) # 
```

There seems to be a negative relationship but it's hard to tell because of overplotting. Let's look at games between strong rivals first. We try a different approach: let's look only at games between strong players. For this, we create the var "strength" which is the sum of the ratings of both players. Then we repeat the scatterplot but this time only for games where the sum of the players > 4000 (i.e. where players had an average strength of 2000).

```{r warning=FALSE, message=FALSE}
df$strength <- with(df, white_rating + black_rating)
ggplot(df[df$strength > 4000,], aes(x=diff, y=turns)) + geom_point(alpha=0.2) + geom_smooth(method="lm")
```

Now we see a bit better that games where the rating difference is over 800 points seldom last more than 100 turns, whereas this is very common for games between similarly-rated rivals. But it's still somewhat murky. Another strategy is to turn rating difference into a categorical variable and plot a bar chart. Let's try this approach.

```{r fig.height=5}
# Create categorical variable of rating difference as diffc
df$diff_cat <- cut(df$diff, 
                   breaks=c(-Inf, 400, 800, Inf), 
                   labels=c("Less than 400","400 to 800","Over 800"))

# Group by diffc and estimate mean turns for each category of diffc, then plot 
df %>% 
  group_by(diff_cat) %>% 
  summarize(avg_turn = mean(turns)) %>% 
  ggplot(aes(x=diff_cat, y=avg_turn)) + 
  geom_col(fill="steelblue") + 
  geom_text(aes(label=round(avg_turn, digits=1)), position=position_dodge(width=0.9), vjust=-0.25) + 
  xlab("Absolute rating difference between players") + 
  ylab("Average number of turns")
```

Now we see much more clearly that games between similarly strong rivals are longer than games between unequal rivals.

## 4. How does the rating difference between the two players predict the outcome of the game?

The rating systems used in lichess is [Glicko 2](https://en.wikipedia.org/wiki/Glicko_rating_system). Just as in the [ELO](https://en.wikipedia.org/wiki/Elo_rating_system) ratings used by International Federation of Chess (FIDE), Glicko 2 ratings indicate the relative strength of players in a given player pool. The rating difference between two players predicts that likelihood that each of them will win. This is the theory. But do we see this in the data?
For this, we're going to 'cut' our rating difference variable into segments, so that we can then estimate the average expected score by category of rating difference. Positive values indicate that white has a higher rating, whereas negative values indicate that black was stronger.

```{r}
df$diff <- with(df, white_rating - black_rating)
hist(df$diff, breaks=50)
df$diffc <- with(df, 
                     cut(diff,
                         breaks = c(-Inf, -500, -400, -300, -200, -100, 0, 100, 200, 300, 400, 500, Inf),
                         right  = FALSE))

```
Now we need to create a numeric result variable that takes the value 1 when white wins, 0.5 when it's a draw and 0 when black wins.

```{r}
table(df$winner)
df$outcome <- as.numeric(ifelse(df$winner=="white", 1,
                                ifelse(df$winner=="black", 0, 0.5)))

table(df$outcome)
```

And now we can finally plot the expected results versus the rating difference:

```{r}
df %>% 
  group_by(diffc) %>% 
  summarize(mean_score = mean(outcome, na.rm=TRUE)) %>% 
  ggplot(aes(x=diffc, y=mean_score)) + geom_col()
```

## 5. What predicts whether a game will end by checkmate, resignation or time out

Now we turn to the most interesting question. In chess, a game can either be a draw or have a decisive result. Games with decisive results can can end (a) with checkmate, (b) with a resignation, or (c) with a time-out (a player runs out of time and loses the game). What affects the type of 'ending' that a game will have? My intuition is that games between stronger players are more likely to end when someone resigns, because stronger players are better at evaluating their chances at winning a losing battle. First, let's see if players' rating difference is related to how the games end:

```{r message=FALSE}
# Create variable "absolute rating difference"
df$abs_diff <- with(df, sqrt((white_rating - black_rating)^2))

# "Cut" absolute rating difference to make it categorical
df$abs_diffc <- with(df, 
                     cut(diff,
                         breaks = c(0, 100, 200, 300, 400, 500, Inf),
                         right  = FALSE))

# Plot outcome vs players' rating difference
df %>% 
  group_by(abs_diffc, victory_status) %>% 
  summarize(n = n()) %>% 
  mutate(prop = n/sum(n)) %>%
  ggplot(aes(fill=victory_status, y=prop, x=abs_diffc)) + 
    geom_bar(position="fill", stat="identity")
  
```
There isn't a clear pattern. For each category of rating difference, the proportions are about the same.
Now we will train a Machine Learning algorithm to predict when a player will resign versus suffer check mate. For that, we will subset games that end with one of those two outcomes:

```{r message=FALSE}
# Get rid of draws and timeouts
df2 <- filter(df, victory_status %in% c("mate", "resign"))
df2$victory_status <- droplevels(df2$victory_status)

# Remove NA's
df2 <- df2 [complete.cases(df2), ]

# We split the dataset into a training dataset and a test dataset
smp_size <- floor(0.75 * nrow(df2))
set.seed(123)
train_ind <- sample(seq_len(nrow(df2)), size = smp_size)
train_data <- df2[train_ind, ]
test_data <- df2[-train_ind, ]

# Estimate random tree model on train dataset
library(randomForest)
rt_model <- randomForest(victory_status ~ ., data = train_data)

# Now see if it can predict the outcomes in the test dataset
test_data$pred <- predict(rt_model, test_data, type = "class")
mean(test_data$pred == test_data$victory_status)
```
The model can predict the outcome of the game (whether a player will resign or be checkmated) around 70% of the time, which is much better than random!